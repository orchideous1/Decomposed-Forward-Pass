{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b45f131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linyiwu/anaconda3/envs/omnizip/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "Qwen2_5OmniToken2WavModel must inference with fp32, but flash_attention_2 only supports fp16 and bf16, attention implementation of Qwen2_5OmniToken2WavModel will fallback to sdpa.\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:04<00:00,  1.01it/s]\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from DePass import decomposed_state_manager\n",
    "from DePass.utils import from_states_to_probs\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,  Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "model_path = \"/ssd/linyiwu/Qwen2.5-Omni-7B/\" # The path to model\n",
    "model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation='flash_attention_2'\n",
    "    \n",
    ")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = model.eval() \n",
    "processor = Qwen2_5OmniProcessor.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ba4a85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 attention swapped to Qwen2_5OmniSdpaAttention\n",
      "Layer 1 attention swapped to Qwen2_5OmniSdpaAttention\n",
      "Layer 2 attention swapped to Qwen2_5OmniSdpaAttention\n",
      "Layer 3 attention swapped to Qwen2_5OmniSdpaAttention\n",
      "Layer 4 attention swapped to Qwen2_5OmniSdpaAttention\n",
      "Layer 5 attention swapped to Qwen2_5OmniSdpaAttention\n",
      "Layer 6 attention swapped to Qwen2_5OmniSdpaAttention\n",
      "Layer 7 attention swapped to Qwen2_5OmniSdpaAttention\n",
      "Layer 8 attention swapped to Qwen2_5OmniSdpaAttention\n",
      "Layer 9 attention swapped to Qwen2_5OmniSdpaAttention\n",
      "Layer 10 attention swapped to Qwen2_5OmniSdpaAttention\n",
      "Layer 11 attention swapped to Qwen2_5OmniSdpaAttention\n",
      "Layer 12 attention swapped to Qwen2_5OmniSdpaAttention\n",
      "Layer 13 attention swapped to Qwen2_5OmniSdpaAttention\n",
      "Layer 14 attention swapped to Qwen2_5OmniSdpaAttention\n",
      "Layer 15 attention swapped to Qwen2_5OmniSdpaAttention\n",
      "Layer 16 attention swapped to Qwen2_5OmniSdpaAttention\n",
      "Layer 17 attention swapped to Qwen2_5OmniSdpaAttention\n",
      "Layer 18 attention swapped to Qwen2_5OmniSdpaAttention\n",
      "Layer 19 attention swapped to Qwen2_5OmniSdpaAttention\n",
      "Layer 20 attention swapped to Qwen2_5OmniSdpaAttention\n",
      "Layer 21 attention swapped to Qwen2_5OmniSdpaAttention\n",
      "Layer 22 attention swapped to Qwen2_5OmniSdpaAttention\n",
      "Layer 23 attention swapped to Qwen2_5OmniSdpaAttention\n",
      "Layer 24 attention swapped to Qwen2_5OmniSdpaAttention\n",
      "Layer 25 attention swapped to Qwen2_5OmniSdpaAttention\n",
      "Layer 26 attention swapped to Qwen2_5OmniSdpaAttention\n",
      "Layer 27 attention swapped to Qwen2_5OmniSdpaAttention\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"/home/linyiwu/OmniZip\")\n",
    "from omnizip.note_modeling_qwen2_5_omni import (\n",
    "    Qwen2_5OmniAttention,       # Eager\n",
    "    Qwen2_5OmniFlashAttention2, # Flash Attention 2\n",
    "    Qwen2_5OmniSdpaAttention    # SDPA\n",
    ")\n",
    "def swap_attention_module(module, layer_idx, replacing_attn):\n",
    "    old_attn = module.self_attn\n",
    "    old_config = module.self_attn.config\n",
    "    current_device = old_attn.q_proj.weight.device\n",
    "    current_dtype = old_attn.q_proj.weight.dtype\n",
    "\n",
    "    new_attn = replacing_attn(old_config, layer_idx=layer_idx)\n",
    "    new_attn.load_state_dict(old_attn.state_dict())\n",
    "    new_attn.to(current_device, dtype=current_dtype)\n",
    "    module.self_attn = new_attn\n",
    "    print(f\"Layer {layer_idx} attention swapped to {replacing_attn.__name__}\")\n",
    "\n",
    "\n",
    "for layer_idx, layer in enumerate(model.thinker.model.layers):\n",
    "    swap_attention_module(layer, layer_idx, Qwen2_5OmniSdpaAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a96d0e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "\n",
    "video_path = \"/ssd/linyiwu/worldsense/videos/FnOIAada.mp4\"\n",
    "question = \"What is the number on the rear wing of the red vehicle at the start of the video?\"\n",
    "task_candidates = [               \n",
    "    \"A. 56.\",\n",
    "    \"B. 50.\",\n",
    "    \"C. 58.\",\n",
    "    \"D. 59.\"\n",
    "]\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}]},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"video\", \"video\": video_path},\n",
    "            {\"type\": \"text\", \"text\": f\"{question}\\nChoose from the following options:\\n\" + \"\\n\".join(task_candidates) + \"\\nPlease provide your answer by selecting the corresponding option and give your answer straight away.\"},\n",
    "        ],\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "133924ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DePass import qwenOmnimanager\n",
    "# print(type(decomposed_state_manager))\n",
    "# print(type(qwenOmnimanager))\n",
    "# with open(\"/home/linyiwu/OmniZip/print_result.md\", \"w\") as f:\n",
    "#     print(model.thinker.config.text_config, file=f)\n",
    "\n",
    "DecomposedStateManager = qwenOmnimanager(model, processor, mlp_decomposed_function=\"softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "359ce471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2_5OmniTextConfig {\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 3584,\n",
       "  \"init_std\": 0.02,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 18944,\n",
       "  \"max_position_embeddings\": 32768,\n",
       "  \"max_window_layers\": 28,\n",
       "  \"model_type\": \"qwen2_5_omni_text\",\n",
       "  \"num_attention_heads\": 28,\n",
       "  \"num_hidden_layers\": 28,\n",
       "  \"num_key_value_heads\": 4,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": {\n",
       "    \"mrope_section\": [\n",
       "      16,\n",
       "      24,\n",
       "      24\n",
       "    ],\n",
       "    \"rope_type\": \"default\",\n",
       "    \"type\": \"default\"\n",
       "  },\n",
       "  \"rope_theta\": 1000000.0,\n",
       "  \"sliding_window\": 32768,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"transformers_version\": \"4.52.3\",\n",
       "  \"use_cache\": true,\n",
       "  \"use_sliding_window\": false,\n",
       "  \"vocab_size\": 152064\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.thinker.model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad853626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|im_start|>system\\nYou are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.<|im_end|>\\n<|im_start|>user\\n<|vision_bos|><|VIDEO|><|vision_eos|>What is the number on the rear wing of the red vehicle at the start of the video?\\nChoose from the following options:\\nA. 56.\\nB. 50.\\nC. 58.\\nD. 59.\\nPlease provide your answer by selecting the corresponding option and give your answer straight away.<|im_end|>\\n<|im_start|>assistant\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linyiwu/OmniZip/qwen-omni-utils/src/qwen_omni_utils/v2_5/audio_process.py:85: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  librosa.load(\n",
      "/home/linyiwu/anaconda3/envs/omnizip/lib/python3.10/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "qwen-vl-utils using decord to read video.\n",
      "Unused or unrecognized kwargs: return_tensors, images.\n",
      "Computing Audio:   0%|          | 0/600 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 23.52 GiB of which 1.42 GiB is free. Including non-PyTorch memory, this process has 22.07 GiB memory in use. Of the allocated memory 21.02 GiB is allocated by PyTorch, and 610.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m states, attribute_state, mod_names \u001b[38;5;241m=\u001b[39m \u001b[43mDecomposedStateManager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_last_layer_decomposed_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Decomposed-Forward-Pass/DePass/qwenOmnimanager.py:264\u001b[0m, in \u001b[0;36mqwenOmnimanager.get_last_layer_decomposed_state\u001b[0;34m(self, conversation, single_compute_token)\u001b[0m\n\u001b[1;32m    262\u001b[0m states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# 修改调用为 compute_modality_decomposed_state，并传入 input_ids\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m attribute_state, mod_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_modality_decomposed_state\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_layer_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43msingle_compute_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msingle_compute_token\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m states, attribute_state, mod_names\n",
      "File \u001b[0;32m~/Decomposed-Forward-Pass/DePass/qwenOmnimanager.py:354\u001b[0m, in \u001b[0;36mqwenOmnimanager.compute_modality_decomposed_state\u001b[0;34m(self, input_ids, start_layer_idx, single_compute_token)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_layer_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;66;03m# Attention 分解\u001b[39;00m\n\u001b[1;32m    353\u001b[0m     s_attn \u001b[38;5;241m=\u001b[39m states[layer_idx\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m--> 354\u001b[0m     attribute_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn_decomposed_compute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropouts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcausals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattribute_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;66;03m# MLP 分解\u001b[39;00m\n\u001b[1;32m    356\u001b[0m     s_mlp \u001b[38;5;241m=\u001b[39m states[layer_idx\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/Decomposed-Forward-Pass/DePass/qwenOmnimanager.py:430\u001b[0m, in \u001b[0;36mqwenOmnimanager._attn_decomposed_compute\u001b[0;34m(self, layer_idx, state, masks, dropouts, causals, attribute_state)\u001b[0m\n\u001b[1;32m    428\u001b[0m attribute_values \u001b[38;5;241m=\u001b[39m repeat_kv(attribute_values, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads)\n\u001b[1;32m    429\u001b[0m vo_attribute \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mknqh,jnh->kqnj\u001b[39m\u001b[38;5;124m\"\u001b[39m, attribute_values, o_proj_weight)\n\u001b[0;32m--> 430\u001b[0m attribute_state \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miknd, nqk -> iqd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvo_attribute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attribute_state\n",
      "File \u001b[0;32m~/anaconda3/envs/omnizip/lib/python3.10/site-packages/torch/functional.py:373\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 373\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    375\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 23.52 GiB of which 1.42 GiB is free. Including non-PyTorch memory, this process has 22.07 GiB memory in use. Of the allocated memory 21.02 GiB is allocated by PyTorch, and 610.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "states, attribute_state, mod_names = DecomposedStateManager.get_last_layer_decomposed_state(conversation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e36c535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(35, ['D'], 0.1572265625),\n",
       " (34, ['C'], 0.138671875),\n",
       " (11908, ['Oh'], 0.12890625),\n",
       " (80022, ['Hmm'], 0.12890625),\n",
       " (32, ['A'], 0.12890625),\n",
       " (785, ['The'], 0.08349609375),\n",
       " (2132, ['It'], 0.07373046875),\n",
       " (33, ['B'], 0.0693359375),\n",
       " (24765, ['Ah'], 0.0306396484375),\n",
       " (18665, ['Hey'], 0.02392578125)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "last_layer_state = states[-1]\n",
    "state = DecomposedStateManager.text_model.norm(last_layer_state[0, -1, :])\n",
    "def from_states_to_probs(manager, states, topk=5):\n",
    "    \"\"\"convert states(d dimension)to token ids and probabilities(vocabulary size)\n",
    "    Args:\n",
    "        states: torch.Tensor, shape=(d, )\n",
    "        tokenizer: transformers.Tokenizer\n",
    "        layer_idx: int, layer index\n",
    "        topk: int, topk\n",
    "    Returns:\n",
    "        token_probs: dict, token probabilities\n",
    "    \"\"\"\n",
    "    logits = manager.model.thinker.lm_head(states)\n",
    "    traj_log_probs = logits.log_softmax(dim=-1).squeeze()\n",
    "    topk_values, topk_indices = torch.topk(traj_log_probs, k=topk)\n",
    "    probs = torch.exp(topk_values)\n",
    "    token_probs = []\n",
    "    for idx, prob in zip(topk_indices.cpu(), probs.cpu()):\n",
    "        token = manager.processor.batch_decode(\n",
    "                    [idx],\n",
    "                )\n",
    "\n",
    "        token_probs.append((idx.item(), token, prob.item()))\n",
    "    return token_probs\n",
    "\n",
    "probs = from_states_to_probs(DecomposedStateManager, state, topk=10)\n",
    "probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b094be4",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnizip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
