{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650654fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 定义文件路径\n",
    "file_path = \"/root/cjiang/info_flow/Context-Cite/input_level_decompose/subspace_decompose/language_probing/results/llama-3.1-8b-instruct_counterfact_results.pt\"\n",
    "\n",
    "# 加载数据\n",
    "data_all = torch.load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458edaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 初始化存储不同语言的embedding的字典\n",
    "lang_embeddings = {\n",
    "    'French': [],\n",
    "    'German': [],\n",
    "    'Italian': [],\n",
    "    'Portuguese': [],\n",
    "    'Spanish': [],\n",
    "    'original': []\n",
    "}\n",
    "\n",
    "# 遍历所有数据，收集每种语言的embeddings\n",
    "for data in data_all:\n",
    "    for lang in lang_embeddings.keys():\n",
    "        if lang in data and 'lang_embedding' in data[lang]:\n",
    "            # 如果是tensor则转换为numpy\n",
    "            if isinstance(data[lang]['lang_embedding'], torch.Tensor):\n",
    "                embedding = data[lang]['lang_embedding'].numpy()\n",
    "            else:\n",
    "                embedding = data[lang]['lang_embedding']\n",
    "            lang_embeddings[lang].append(embedding)\n",
    "\n",
    "# 将列表转换为numpy数组\n",
    "for lang in lang_embeddings:\n",
    "    lang_embeddings[lang] = np.stack(lang_embeddings[lang])\n",
    "\n",
    "# 打印每种语言的embedding形状\n",
    "for lang, embeddings in lang_embeddings.items():\n",
    "    print(f\"{lang} embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee93fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "def plot_tsne_with_metrics(lang_embeddings, selected_languages=None):\n",
    "    \"\"\"\n",
    "    对选定语言的embeddings进行TSNE可视化并计算聚类评估指标\n",
    "    \"\"\"\n",
    "    if selected_languages is None:\n",
    "        selected_languages = list(lang_embeddings.keys())\n",
    "    \n",
    "    for lang in selected_languages:\n",
    "        if lang not in lang_embeddings:\n",
    "            raise ValueError(f\"Language {lang} not found in embeddings\")\n",
    "    \n",
    "    all_embeddings = []\n",
    "    labels = []\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown']\n",
    "    \n",
    "    # 创建显示标签的映射\n",
    "    display_names = {\n",
    "        'original': 'English',  # 将'original'映射为'English'\n",
    "        'French': 'French',\n",
    "        'German': 'German',\n",
    "        'Italian': 'Italian',\n",
    "        'Portuguese': 'Portuguese',\n",
    "        'Spanish': 'Spanish'\n",
    "    }\n",
    "    \n",
    "    lang_to_color = dict(zip(selected_languages, colors[:len(selected_languages)]))\n",
    "    \n",
    "    # 准备数据和标签\n",
    "    for lang in selected_languages:\n",
    "        all_embeddings.append(lang_embeddings[lang])\n",
    "        labels.extend([lang] * len(lang_embeddings[lang]))\n",
    "    \n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # 使用TSNE进行降维\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    embeddings_2d = tsne.fit_transform(all_embeddings)\n",
    "    \n",
    "    # 计算评估指标\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    numeric_labels = le.fit_transform(labels)\n",
    "    \n",
    "    metrics = {\n",
    "        'silhouette': silhouette_score(embeddings_2d, numeric_labels),\n",
    "        'davies_bouldin': davies_bouldin_score(embeddings_2d, numeric_labels),\n",
    "        'calinski_harabasz': calinski_harabasz_score(embeddings_2d, numeric_labels)\n",
    "    }\n",
    "    \n",
    "    # 设置图片大小和字体大小\n",
    "    plt.figure(figsize=(6, 4))  # 减小图片大小\n",
    "    plt.rcParams.update({'font.size': 12})  # 增加基础字体大小\n",
    "    \n",
    "    # 绘制散点图\n",
    "    for lang in selected_languages:\n",
    "        mask = labels == lang\n",
    "        plt.scatter(\n",
    "            embeddings_2d[mask, 0],\n",
    "            embeddings_2d[mask, 1],\n",
    "            c=lang_to_color[lang],\n",
    "            label=display_names[lang],  # 使用显示名称\n",
    "            alpha=0.6,\n",
    "            s=30  # 增加点的大小\n",
    "        )\n",
    "    \n",
    "    plt.legend(fontsize=14)  # 增加图例字体大小\n",
    "    plt.title('TSNE visualization of language embeddings', fontsize=20)  # 增加标题字体大小\n",
    "    plt.xlabel('TSNE dimension 1', fontsize=18)  # 增加x轴标签字体大小\n",
    "    plt.ylabel('TSNE dimension 2', fontsize=18)  # 增加y轴标签字体大小\n",
    "    \n",
    "    # 调整布局以防止文字重叠\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 打印评估指标\n",
    "    print(\"Clustering Evaluation Metrics:\")\n",
    "    print(f\"Silhouette Score: {metrics['silhouette']:.3f}\")\n",
    "    print(f\"Davies-Bouldin Score: {metrics['davies_bouldin']:.3f}\")\n",
    "    print(f\"Calinski-Harabasz Score: {metrics['calinski_harabasz']:.3f}\")\n",
    "    \n",
    "    return metrics\n",
    "# 使用示例：\n",
    "selected_langs = ['French', 'German','original']\n",
    "metrics = plot_tsne_with_metrics(lang_embeddings, selected_langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85762acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_tsne(lang_embeddings, selected_languages=None):\n",
    "    \"\"\"\n",
    "    对选定语言的embeddings进行TSNE可视化\n",
    "    Args:\n",
    "        lang_embeddings: 包含所有语言embeddings的字典\n",
    "        selected_languages: 要可视化的语言列表，如果为None则使用所有语言\n",
    "    \"\"\"\n",
    "    # 如果没有指定语言，使用所有语言\n",
    "    if selected_languages is None:\n",
    "        selected_languages = list(lang_embeddings.keys())\n",
    "    \n",
    "    # 检查选择的语言是否有效\n",
    "    for lang in selected_languages:\n",
    "        if lang not in lang_embeddings:\n",
    "            raise ValueError(f\"Language {lang} not found in embeddings\")\n",
    "    \n",
    "    # 准备数据\n",
    "    all_embeddings = []\n",
    "    labels = []\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown']\n",
    "    lang_to_color = dict(zip(selected_languages, colors[:len(selected_languages)]))\n",
    "    \n",
    "    # 只处理选定的语言\n",
    "    for lang in selected_languages:\n",
    "        all_embeddings.append(lang_embeddings[lang])\n",
    "        labels.extend([lang] * len(lang_embeddings[lang]))\n",
    "    \n",
    "    # 将所有embeddings拼接成一个大矩阵\n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    \n",
    "    # 使用TSNE进行降维\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    embeddings_2d = tsne.fit_transform(all_embeddings)\n",
    "    \n",
    "    # 绘制散点图\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for lang in selected_languages:\n",
    "        mask = np.array(labels) == lang\n",
    "        plt.scatter(\n",
    "            embeddings_2d[mask, 0],\n",
    "            embeddings_2d[mask, 1],\n",
    "            c=lang_to_color[lang],\n",
    "            label=lang,\n",
    "            alpha=0.6\n",
    "        )\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title('TSNE visualization of selected language embeddings')\n",
    "    plt.xlabel('TSNE dimension 1')\n",
    "    plt.ylabel('TSNE dimension 2')\n",
    "    plt.show()\n",
    "\n",
    "# 使用示例：\n",
    "# 选择特定语言进行可视化\n",
    "selected_langs = ['French', 'German', 'Spanish']  # 可以根据需要选择语言\n",
    "plot_tsne(lang_embeddings, selected_langs)\n",
    "\n",
    "# 或者可视化所有语言\n",
    "# plot_tsne(lang_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2c0f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cite_functions import attr_state_manager\n",
    "import cite_functions as cf\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "from tuned_lens.plotting import PredictionTrajectory\n",
    "from tuned_lens.nn.lenses import LogitLens\n",
    "model_path = r\"/root/models/llama_3_1_8b_instruct/\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca038417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_predictions(model, tokenizer, state, k=5):\n",
    "    # Get logits from combined attribute states\n",
    "    logits = model.lm_head(state)\n",
    "    traj_log_probs = torch.from_numpy(\n",
    "        logits.log_softmax(dim=-1).squeeze().detach().cpu().numpy()\n",
    "    )\n",
    "    topk_indices = torch.topk(traj_log_probs, k=k)\n",
    "    probs = torch.exp(traj_log_probs[topk_indices.indices])\n",
    "    token_probs = []\n",
    "    for idx, prob in zip(topk_indices.indices, probs):\n",
    "        token = tokenizer.decode(idx)\n",
    "        token_probs.append((idx.item(), token, prob.item()))\n",
    "    \n",
    "    return token_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5890f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_state_predictions(model, tokenizer, data_all[4]['German']['lang_embedding'], k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22280c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_state_predictions(model, tokenizer, data_all[4]['German']['semantic_embedding'], k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92006c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "languages = ['original', 'French', 'German', 'Italian', 'Portuguese', 'Spanish']\n",
    "\n",
    "for data in data_all:\n",
    "    sample_result = {}\n",
    "    for lang in languages:\n",
    "        if lang in data:\n",
    "            lang_result = {\n",
    "                'prompt': data[lang]['prompt'],\n",
    "                'answer': data[lang]['answer']\n",
    "            }\n",
    "            \n",
    "            # 获取语言嵌入的预测\n",
    "            if 'lang_embedding' in data[lang]:\n",
    "                lang_preds = get_state_predictions(model, tokenizer, data[lang]['lang_embedding'], k=5)\n",
    "                lang_result['lang_predictions'] = lang_preds\n",
    "            \n",
    "            # 获取语义嵌入的预测\n",
    "            if 'semantic_embedding' in data[lang]:\n",
    "                semantic_preds = get_state_predictions(model, tokenizer, data[lang]['semantic_embedding'], k=5)\n",
    "                lang_result['semantic_predictions'] = semantic_preds\n",
    "            \n",
    "            sample_result[lang] = lang_result\n",
    "    \n",
    "    results.append(sample_result)\n",
    "\n",
    "# 打印示例结果\n",
    "print(\"Sample results for first data point:\")\n",
    "for lang in languages:\n",
    "    if lang in results[0]:\n",
    "        print(f\"\\n{lang}:\")\n",
    "        print(f\"Prompt: {results[0][lang]['prompt']}\")\n",
    "        print(f\"Answer: {results[0][lang]['answer']}\")\n",
    "        print(\"Language embedding predictions:\")\n",
    "        for token_id, token, prob in results[0][lang]['lang_predictions']:\n",
    "            print(f\"  {token}: {prob:.4f}\")\n",
    "        print(\"Semantic embedding predictions:\")\n",
    "        for token_id, token, prob in results[0][lang]['semantic_predictions']:\n",
    "            print(f\"  {token}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cefdc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bd44bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('predictions.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e92563",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c520844",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all[4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info_detect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
